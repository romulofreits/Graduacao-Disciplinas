x = c(1, 2, 3, 4, 5)
y = c(3, 7.5, 7, 11.5, 11)
m = lm(y~x)
x = c(1, 2, 3, 4, 5)
m = lm(y~x)
summary(m)
anova(m)
x = c(0, 2, 4, 6, 8)
y = c(2, 3, 14, 15, 26)
m = lm(y~x)
summary(m)
-3/(sqrt(10**(1/5)+(16/40)))
qt(0.95, 3)
pt(0.95, 3)
qt(0.05, 3)
dados = scan()
dados
# Analise descritiva dos dados
hist(dados)
qqplot(dados)
boxplot(dados)
install.packages('readr')
library(readr)
summary(dados)
curt = kurtosis(dados)
sss = skewness(dados)
sd = sd(dados)
library(readr)
curt = kurtosis(dados)
sss = skewness(dados)
install.packages('readr')
install.packages("readr")
library(readr)
install.packages('e1071')
library(e1071)
install.packages('dplyr')
library(dplyr)
summary(dados)
curt = kurtosis(dados)
sss = skewness(dados)
sd = sd(dados)
curt
sss
sd
# TESTES NAO PARAMETRICOS
shapiro.test(dados)
library(nortest)
lillie.test(dados)
nLL0 <- function(shape, scale) -sum(dweibull(dados, shape, scale, log = TRUE))
fit0 <- stats4::mle(nLL1, start = list(shape = 1, scale = 1), nobs = length(dados))
fit0 <- stats4::mle(nLL0, start = list(shape = 1, scale = 1), nobs = length(dados))
fit0
install.packages('extraDistr')
library(extraDistr)
nLL1 <- function(alpha, beta) - sum(dfatigue(dados1, alpha, beta, log = TRUE))
nLL1 <- function(alpha, beta) - sum(dfatigue(dados, alpha, beta, log = TRUE))
tbar <- mean(dados)
hbar <- 1/mean(1/dados)
alphamm <- sqrt(2*(sqrt(tbar/hbar)-1)); alphamm
betamm <- sqrt(tbar*hbar);betamm
fit2 <- stats4::mle(nLL1, start = list(alpha =alphamm, beta = betamm),
nobs = length(dados), method = "BFGS")
fit2
goftest::ad.test(x = dados, null = "pfatigue", alpha = 0.2743686,
beta = 160.0504747, estimated = TRUE)
install.packages('goftest')
library(goftest)
goftest::ad.test(x = dados, null = "pfatigue", alpha = 0.2743686,
beta = 160.0504747, estimated = TRUE)
hist(dados)
x <- seq(100, 300, by = 10)
hist(dados, freq = F)
lines(x, dweibull(x, 2.940806,247.052834), col= 2)
lines(x, dweibull(x, 0.2743686,160.0504747), col= 2)
lines(x, dfatigue(x, 0.2743686, 160.0504747), col= 3)
lines(x, dfatigue(x, 0.2743686, 160.0504747), col= 'blue')
lines(x, dweibull(x, 0.2743686,160.0504747), col = 'red')
x <- seq(90, 300, by = 10)
hist(dados, freq = F)
lines(x, dweibull(x, 0.2743686,160.0504747), col = 'red')
lines(x, dfatigue(x, 0.2743686, 160.0504747), col= 'blue')
lines(x, dfatigue(x, 0.2743686, 160.0504747), col= 'blue', lwd = 3)
lines(x, dfatigue(x, 0.2743686, 160.0504747), col= 'blue', lwd = 2)
# Estimando os parametros de uma distribuicao Weibull:
nLL0 <- function(shape, scale) -sum(dweibull(dados, shape, scale, log = TRUE))
fit0 <- stats4::mle(nLL1, start = list(shape = 1, scale = 1), nobs = length(dados))
fit0
n = c(1, 2, 3, 4, 5, 6, 7, 8, 9 ,10)
mean(n)
sd(n)
sum(n)
rm(list = ls())
rm(list = ls())
setwd("C:/Users/romul/OneDrive/Área de Trabalho/Semestre 2023.2/Estatística Não Paramétrica/Trabalho1_NP")
# lendo o arquivo .txt
arquivo = readLines('Dados_Equipe_n1.txt', warn = FALSE)
# separando os dados para analise
encontrar = which(arquivo == "")  # encontre o indice da linha em branco
if (length(encontrar) >= 1) {
parte1 = arquivo[1:(encontrar - 1)]  # primeira parte
parte2 = arquivo[(encontrar + 1):length(arquivo)]  # segunda parte
} else {
cat("Não foi possível encontrar a linha em branco de separação.")
}
valores = paste(parte2, collapse = ",")
valores_sep = unlist(strsplit(valores, ","))
dados = as.numeric(valores_sep)
dados = sort(dados)
setwd("C:/Users/romul/OneDrive/Área de Trabalho/Semestre 2023.2/Estatística Não Paramétrica/Trabalho1_NP")
knitr::opts_chunk$set(echo = FALSE)
setwd("C:/Users/romul/OneDrive/Área de Trabalho/Semestre 2023.2/Estatística Não Paramétrica/Trabalho1_NP")
x = seq(80, 300, by = 10)
hist(dados, freq = F, main = 'Histograma dos dados', col = 'azure3',
xlab = '', ylab = '')
lines(x, dweibull(x, 3.726646, 183.660840), col = 'red')
#lines(x, density(dist1), col = 'blue')
legend("topright", legend = c('Weibull', 'Dist2'),
col = c('red', 'blue'), lwd = 5)
dados.equipe1 = data.frame(dados)
medidas.descritivas = dados.equipe1 %>%
summarise(
media = mean(dados),
q1 = quantile(dados, probs = 0.25),
mediana = median(dados),
q3 = quantile(dados, probs = 0.75),
desv.pd = sd(dados),
minimo = min(dados),
maximo = max(dados),
curt = kurtosis(dados),
assi = skewness(dados)
)
# pacotes necessarios:
library(readr)
library(e1071)
library(dplyr)
library(nortest)
library(extraDistr) # outras distribuicoes
library(goftest) # teste de anderson-darling e cramer-von mises
dados.equipe1 = data.frame(dados)
medidas.descritivas = dados.equipe1 %>%
summarise(
media = mean(dados),
q1 = quantile(dados, probs = 0.25),
mediana = median(dados),
q3 = quantile(dados, probs = 0.75),
desv.pd = sd(dados),
minimo = min(dados),
maximo = max(dados),
curt = kurtosis(dados),
assi = skewness(dados)
)
# pacotes necessarios:
library(readr)
library(e1071)
library(dplyr)
library(nortest)
library(extraDistr) # outras distribuicoes
library(goftest) # teste de anderson-darling e cramer-von mises
#Análise Exploratória dos Dados:
dados.equipe1 = data.frame(dados)
medidas.descritivas = dados.equipe1 %>%
summarise(
media = mean(dados),
q1 = quantile(dados, probs = 0.25),
mediana = median(dados),
q3 = quantile(dados, probs = 0.75),
desv.pd = sd(dados),
minimo = min(dados),
maximo = max(dados),
curt = kurtosis(dados),
assi = skewness(dados)
)
# pacotes necessarios:
library(readr)
library(e1071)
library(dplyr)
library(nortest)
library(extraDistr) # outras distribuicoes
library(goftest) # teste de anderson-darling e cramer-von mises
#Análise Exploratória dos Dados:
dados.equipe1 = data.frame(dados)
medidas.descritivas = dados.equipe1 %>%
summarise(
media = mean(dados),
q1 = quantile(dados, probs = 0.25),
mediana = median(dados),
q3 = quantile(dados, probs = 0.75),
desv.pd = sd(dados),
minimo = min(dados),
maximo = max(dados),
curt = kurtosis(dados),
assi = skewness(dados)
)
View(medidas.descritivas)
dados
# pacotes necessarios:
library(readr)
library(e1071)
library(dplyr)
library(nortest)
library(extraDistr) # outras distribuicoes
library(goftest) # teste de anderson-darling e cramer-von mises
#Análise Exploratória dos Dados:
dados.equipe1 = data.frame(dados)
medidas.descritivas = dados.equipe1 %>%
summarise(
media = mean(dados),
q1 = quantile(dados, probs = 0.25),
mediana = median(dados),
q3 = quantile(dados, probs = 0.75),
desv.pd = sd(dados),
minimo = min(dados),
maximo = max(dados),
curt = kurtosis(dados),
assi = skewness(dados)
)
View(medidas.descritivas)
dados = scan()
dados = scan()
dados
dados
# pacotes necessarios:
library(readr)
library(e1071)
library(dplyr)
library(nortest)
library(extraDistr) # outras distribuicoes
library(goftest) # teste de anderson-darling e cramer-von mises
#Análise Exploratória dos Dados:
dados.equipe1 = data.frame(dados)
medidas.descritivas = dados.equipe1 %>%
summarise(
media = mean(dados),
q1 = quantile(dados, probs = 0.25),
mediana = median(dados),
q3 = quantile(dados, probs = 0.75),
desv.pd = sd(dados),
minimo = min(dados),
maximo = max(dados),
curt = kurtosis(dados),
assi = skewness(dados)
)
View(medidas.descritivas)
dados
dados
dados.equipe1
dados = scan()
dados
dados
dados
n = 100
x = rnorm(n, 0, 1)
x
hist(x)
n = 1000
x = rnorm(n, 0, 1)
hist(x)
set.seed(87)
n = 1000
x = rnorm(n, 0, 1)
hist(x)
xbar = mean(x)
s = sd(x)
ci = c(xbar - 1.96*s/sqrt{n}, xbar + 1.96*s/sqrt{n})
ci = c(xbar - 1.96*s/sqrt(n), xbar + 1.96*s/sqrt(n))
ci
# set seed for reproducibility
set.seed(4)
# generate and plot the sample data
Y <- rnorm(n = 100,
mean = 5,
sd = 5)
plot(Y,
pch = 19,
col = "steelblue")
cbind(CIlower = mean(Y) - 1.96 * 5 / 10, CIupper = mean(Y) + 1.96 * 5 / 10)
# set seed
set.seed(1)
# initialize vectors of lower and upper interval boundaries
lower <- numeric(10000)
upper <- numeric(10000)
# loop sampling / estimation / CI
for(i in 1:10000) {
Y <- rnorm(100, mean = 5, sd = 5)
lower[i] <- mean(Y) - 1.96 * 5 / 10
upper[i] <- mean(Y) + 1.96 * 5 / 10
}
# join vectors of interval bounds in a matrix
CIs <- cbind(lower, upper)
dados = iris
summary(dados)
m = lm(iris$Petal.Width ~ iris$Sepal.Length); summary(m)
amostras = numeric(100)
for(i in 1:100){
amostras[i] = list(sample(dados$Sepal.Length, 30, replace = T))
}
amostras
calculo = function(x, y = dados$Petal.Width){
x = unlist(x)
xb = mean(x); Sxx = sum((x - xb)^2); yb = mean(y); Syy = sum((y -yb)^2); Sxy = sum((x - xb)*y)
QMRes = (Syy - Sxy^2/Sxx)/(length(x)-2)
return(c(QMRes, Sxx))
}
calculo(amostras[1])
t = q
Arthur Silva21:12
calculo = function(x){
x = unlist(x)
y = c(m$coe[1] + m$coe[2]*x)
xb = mean(x); Sxx = sum((x - xb)^2); yb = mean(y); Syy = sum((y -yb)^2); Sxy = sum((x - xb)*y)
QMRes = (Syy - Sxy^2/Sxx)/(length(x)-2)
erro = Syy
return(erro)
}
calculo(amostras[1])
t = qt(1-0.05/2,28); t
beta1_est = m$coe[2]; beta1_est
intervalos = numeric()
for(i in 1:10){
print(beta1_est+c(-1,1)*t*calculo(amostras[i]))
}
q
y
calculo = function(x, y = dados$Petal.Width){
x = unlist(x)
xb = mean(x); Sxx = sum((x - xb)^2); yb = mean(y); Syy = sum((y -yb)^2); Sxy = sum((x - xb)*y)
QMRes = (Syy - Sxy^2/Sxx)/(length(x)-2)
return(c(QMRes, Sxx))
}
calculo(amostras[1])
t = q
calculo = function(x){
x = unlist(x)
y = c(m$coe[1] + m$coe[2]*x)
xb = mean(x); Sxx = sum((x - xb)^2); yb = mean(y); Syy = sum((y -yb)^2); Sxy = sum((x - xb)*y)
QMRes = (Syy - Sxy^2/Sxx)/(length(x)-2)
erro = Syy
return(erro)
}
calculo(amostras[1])
t = qt(1-0.05/2,28); t
beta1_est = m$coe[2]; beta1_est
intervalos = numeric()
for(i in 1:10){
print(beta1_est+c(-1,1)*t*calculo(amostras[i]))
}
# Set seed for reproducibility
set.seed(123)
# Step 1: Generate a large sample dataset
n_large <- 1000  # Large dataset size
x_large <- rnorm(n_large, mean = 50, sd = 10)  # Independent variable
y_large <- 2 * x_large + rnorm(n_large, mean = 0, sd = 5)  # Dependent variable
# Step 2: Create multiple random samples
n_samples <- 100  # Number of samples
sample_size <- 50  # Sample size for each sample
sample_indices <- lapply(1:n_samples, function(i) sample(1:n_large, sample_size))
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245)
length(x)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 150)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 200)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 175)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 250)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 250)
length(x)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 250)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 175)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 200)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 150)
mean(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245)
mean(x)
length(x)
summary(x)
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245, 250)
mean(x)
length(x)
summary(x)
sd(x)
mean(x)
ic = mean(x)+c(-1, 1)*sd(x)
ic
x = c(200, 150, 120, 210, 250, 140, 120, 90, 210, 150, 245)
mean(x)
length(x)
summary(x)
sd(x)
ic = mean(x)+c(-1, 1)*sd(x)
ic
amostra = c(118, 167, 72, 79, 76, 106, 102, 113,
73, 119, 162, 114, 120, 93, 135, 147,
77, 157, 115, 88, 152, 70, 65, 91,)
amostra = c(118, 167, 72, 79, 76, 106, 102, 113,
73, 119, 162, 114, 120, 93, 135, 147,
77, 157, 115, 88, 152, 70, 65, 91)
install.packages('DescTools')
library(DescTools)
SignTest()
?SignTest
SignTest(amostra, 'greater', 120, conf.level = 0.975)
SignTest(amostra, alternative = 'greater', 120, conf.level = 0.975)
SignTest(amostra, alternative = 'greater', mu = 120, conf.level = 0.975)
qt(0.10, 49)
?qt
qt(0.90, 49)
qt(0.05, 1)
qt(0.95, 1)
qt(0.95, 1, lower.tail = F)
qt(0.95, 49)
#----------------- Regressao Linear Multipla -----------------------------#
# Passo 1: Carregar os pacotes que serao usados
if(!require(pacman)) install.packages("pacman")
library(pacman)
pacman::p_load(dplyr, car, rstatix, lmtest, ggpubr,
QuantPsyc, psych, scatterplot3d)
# Passo 2: Carregar o banco de dados
# Importante: selecionar o diretorio de trabalho (working directory)
# Isso pode ser feito manualmente: Session > Set Working Directory > Choose Directory
setwd("C:/Users/romul/OneDrive/Área de Trabalho/Semestre 2023.2/Modelos de Regressão I/RegressaoLinearMultipla/CodigosR")
dados = read.csv2('Banco de Dados 12.csv')
View(dados) # visualizacao dos dados em janela separada
glimpse(dados) # visualizacao de um resumo dos dados
# Construcao do modelo:
# Os pressupostos nao sao emcima da variavel dependente, mas sim emcima do residuo do modelo.
mod = lm(Notas ~ Tempo_Rev + Tempo_Sono, dados)
# modelo com interacao
# mod = lm(Notas ~ Tempo_Rev + Tempo_Sono + Tempo_Sono*Tempo_Rev, dados)
# Analise grafica:
par(mfrow=c(2,2)) # todos os graficos devem sair em duas linhas e duas colunas
plot(mod)
par(mfrow=c(1,1))
# Teste de Normalidade dos residuos:
shapiro.test(mod$residuals)
library(nortest)
lillie.test(mod$residuals)
# Outliers nos residuos:
summary(rstandard(mod)) # analisando os residuos padronizados
# esperamos que nao haja nenhum valor fora do intervalo -3 e 3
# Independencia dos residuos (Durbin-Watson):
durbinWatsonTest(mod)
# Homocedasticidade (Breusch-Pagan):
bptest(mod)
# Ausencia de Multicolinearidade:
pairs.panels(dados, hist.col = 'steelblue')
### Multicolinearidade: r > 0.9 (ou 0.8)
vif(mod)
### Multicolinearidade: VIF > 10
# Criacao de um segundo modelo
mod2 = lm(Notas ~ Tempo_Rev, dados)
# Passo 4: Analise do modelo
summary(mod)
summary(mod2)
# Obtencao dos coeficientes padronizados
lm.beta(mod)
lm.beta(mod2)
## Obtencao do IC 95% para os coeficientes
confint(mod)
confint(mod2)
# Comparacao de modelos
## AIC e BIC - Comparacao entre quaisquer modelos
AIC(mod, mod2)
BIC(mod, mod2)
# Para comparacao entre modelos aninhados
anova(mod, mod2)
# O melhor sera o com menor valor de RSS (residual sum of squares)
# Passo 5: Grafico de dispersao
graph = scatterplot3d(dados$Notas ~ dados$Tempo_Rev + dados$Tempo_Sono,
pch = 16, angle = 30, color = "steelblue", box = FALSE,
xlab="Tempo de revisão", ylab="Tempo de sono", zlab="Notas")
graph$plane3d(mod, col="black", draw_polygon = TRUE)
#----------------- Metodos de Selecao de Modelos -----------------------#
pacman::p_load(MASS)
mod.inicial = lm(Notas ~ Tempo_Rev + Tempo_Sono, data = dados)
mod.simples = lm(Notas ~ 1, data = dados)
stepAIC(mod.inicial, scope = list(upper = mod.inicial,
lower = mod.simples), direction = "backward")
# A regressao linear multipla mostrou que o tempo de revisao e o tempo de sono tem influencia sobre as notas. A cada 1 minuto gasto revisando o conteudo, a nota aumenta em media 0,10 (t = 10,005; p < 0,001). Ja a cada hora de sono, a nota aumenta, em media, 0,35 (t = 4,026; p < 0,001).
if(!require(pacman)) install.packages("pacman")
library(pacman)
pacman::p_load(dplyr, car, rstatix, lmtest, ggpubr,
QuantPsyc, psych, scatterplot3d)
dados <- read.csv2('Banco de Dados 12.csv', stringsAsFactors = T,
fileEncoding = "latin1") # Carregamento do arquivo csv
View(dados)                                 # Visualiza??o dos dados em janela separada
glimpse(dados)                              # Visualiza??o de um resumo dos dados
## Constru??o do modelo:
mod <- lm(Notas ~ Tempo_Rev + Tempo_Sono, dados)
## An?lise gr?fica:
par(mfrow=c(2,2))
plot(mod)
## Normalidade dos res?duos:
shapiro.test(mod$residuals)
## Outliers nos res?duos:
summary(rstandard(mod))
## Independ?ncia dos res?duos (Durbin-Watson):
durbinWatsonTest(mod)
## Homocedasticidade (Breusch-Pagan):
bptest(mod)
pairs.panels(dados)
vif(mod)
## An?lise gr?fica:
par(mfrow=c(2,2))
## Independ?ncia dos res?duos (Durbin-Watson):
durbinWatsonTest(mod)
